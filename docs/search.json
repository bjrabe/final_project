[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "We start by loading the necessary packages to be used throughout this document.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "EDA.html#quarto",
    "href": "EDA.html#quarto",
    "title": "EDA",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "EDA.html#running-code",
    "href": "EDA.html#running-code",
    "title": "EDA",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "We start by loading the packages we will need for work on this page.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()"
  },
  {
    "objectID": "Modeling.html#quarto",
    "href": "Modeling.html#quarto",
    "title": "Modeling",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Modeling.html#running-code",
    "href": "Modeling.html#running-code",
    "title": "Modeling",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\nIn this project we are working with a data set about diabetes. Our response variable of interest is no diabetes vs diabetes (which includes pre-diabetes). Also in the data set are 21 other variables which can be used as explanatory/feature variables.\nWe are interested in using a subset of the available variables to create a model which can be used to predict the presence or absence of diabetes. We will not use all 21 variables.\nIn choosing variables, we will select some that seem like they may influence the development of diabetes. This is not mandatory for creating a predictive model; it is just the approach we take here to identify a proper subset of the 21 available variables to use as features.\nThe following provides a list of explanatory variables we will use, a description of each variable, and whether the variable is quantitative or categorical:\n\nBMI: Body Mass Index, which is weight in kg divided by the square of height in meters (quantitative)\nSmoker: Smoked 100 or more cigarettes in lifetime (categorical, levels yes and no)\nPhysActivity: Physical activity in past 30 days, not including job (categorical, levels yes and no)\nFruits: Consume fruit one or more times per day (categorical, levels yes and no)\nVeggies: Consume vegetables one or more times per day (categorical, levels yes and no)\nHvyAlcoholConsump: 14 or more alcoholic drinks per week for a male and 7 or more drinks per week for a female (categorical, levels yes and no)\nDiffWalk: Serious difficulty walking or climbing stairs (categorical, levels yes and no)\nSex: (categorical, levels male and female)\n\nThe purpose of our exploratory data analysis (EDA) is to learn about the distributions of these variables and how they relate to one another. We will do this by way of both graphical and numerical summaries. After we have done this, we will create models to try to predict the presence or absence of diabetes based on observed values of the 8 explanatory variables chosen above. We will fit multiple models and compare them to see which performs best."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\nIn this section we first import the data and save it in a new object.\n\ndm_data &lt;- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNext, we note that our categorical variables are saved as data type double. We will change these to factors with the appropriate levels (discussed above).\n\ndm_data &lt;- dm_data |&gt;\n  mutate(Diabetes = factor(Diabetes_binary, levels = c(0, 1), labels = c('no', 'yes')),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c('no', 'yes')),\n         PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c('no', 'yes')),\n         Fruits = factor(Fruits, levels = c(0,1), labels = c('no', 'yes')),\n         Veggies = factor(Veggies, levels = c(0,1), labels = c('no', 'yes')),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c('no', 'yes')),\n         DiffWalk = factor(DiffWalk, levels = c(0,1), labels = c('no', 'yes')),\n         Sex = factor(Sex, levels = c(0,1), labels = c('female', 'male'))\n         )\n\nFinally, we examine the data to see if there are any missing values.\n\nis.na(dm_data) |&gt;\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income             Diabetes \n                   0                    0 \n\n\nWe see from the output that there are no missing values. Therefore, we are ready to proceed with summarizations."
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\nIn this section we perform EDA by examining numerical and graphical summaries of our explanatory and response variables.\nLet’s get started by simply seeing how many individuals in the data set have diabetes. We will look at this both numerically and graphically.\n\ndm_data |&gt;\n  group_by(Diabetes) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Diabetes  count\n  &lt;fct&gt;     &lt;int&gt;\n1 no       218334\n2 yes       35346\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar() + labs(x = 'Diabetes', title = 'Counts of Diabetes vs. No Diabetes')\n\n\n\n\n\n\n\n\nWe see from the contingency table and bar plot that there are far more people without diabetes than with diabetes.\nNext, let’s examine whether BMI appears similar between those with and without diabetes. From what we know, elevated weight should be a risk factor for diabetes, so we would expect a higher BMI in those with diabetes. Again we will look at both a numerical description (contingency table) and a graphical description.\n\ndm_data |&gt;\n  group_by(Diabetes) |&gt;\n  summarize('mean_BMI' = mean(BMI),\n            'median_BMI' = median(BMI),\n            'sd_BMI' = sd(BMI),\n            'IQR_BMI' = IQR(BMI)\n            )\n\n# A tibble: 2 × 5\n  Diabetes mean_BMI median_BMI sd_BMI IQR_BMI\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 no           27.8         27   6.29       7\n2 yes          31.9         31   7.36       8\n\ndm_data |&gt;\n  ggplot(aes(x = Diabetes, y = BMI)) + geom_boxplot() + labs(x = 'Diabetes', y = 'BMI', title = 'BMI Summary by Diabetes Status')\n\n\n\n\n\n\n\n\nFrom the contingency table, it seems that mean and median BMI are higher in those with diabetes, as expected. The box plot is a bit unusual looking. There are a lot of outliers in each group. Let’s look at a density plot to get a better sense of the distribution of BMI between the two groups.\n\ndm_data |&gt;\n  ggplot(aes(BMI)) + geom_density(aes(fill = Diabetes), alpha = 0.5) + labs(x = 'BMI', title = 'BMI Distribution by Diabetes Status')\n\n\n\n\n\n\n\n\nWe see from the density plot that BMI has a long right tail. This explains why there are so many outliers seen on the box plots. Generally speaking though, the values for BMI seem to be shifted higher for those with diabetes relative to those without diabetes.\nWhat about smoking? It isn’t common wisdom that smoking is a major risk factor for diabetes. Let’s see if diabetes rates are higher in smokers by looking at numerical and graphical summaries.\n\ndm_data |&gt;\n  group_by(Diabetes, Smoker) |&gt;\n  summarize(count = n()) \n\n`summarise()` has grouped output by 'Diabetes'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes [2]\n  Diabetes Smoker  count\n  &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;\n1 no       no     124228\n2 no       yes     94106\n3 yes      no      17029\n4 yes      yes     18317\n\n\nThis contingency table is a bit tough to interpret, since the number of people with diabetes is very dissimilar to the number of people without diabetes. Let’s make it easier on ourselves by creating a bar plot instead.\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = Smoker), position = 'dodge') + labs(x = 'Diabetes Status', title = 'Diabetes Status Counts by Smoking Status')\n\n\n\n\n\n\n\n\nWe see from the plot that if you don’t have diabetes, you are more likely to be a non-smoker than a smoker. However, if you do have diabetes, you are more likely to be a smoker. This could suggest that smoking is a risk factor for diabetes (or diabetes is a risk factor for smoking!). An alternative explanation is that people who smoke are generally less healthy, which may predispose them to developing diabetes independent of their smoking status.\nCan we attenuate the relationship between smoking and diabetes by incorporating another risk factor which represents “unhealthiness?” Let’s try by creating another bar plot of diabetes and smoking status, this time faceted by physical activity.\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = Smoker), position = 'dodge') + facet_wrap(~PhysActivity) + labs(x = 'Diabetes Status', title = 'Diabetes Status Counts by Smoking Status and Physical Activity')\n\n\n\n\n\n\n\n\nWe see from the plot that for those who had not done physical activity in the past 30 days, smoking no longer seems to be related to the presence of diabetes! As suspected, this may provide some evidence that general “unhealthiness” is a big player in the association between smoking and diabetes. We do however see in those who had done physical activity in the past 30 days that the relationship between smoking and diabetes persists.\nWhat about fruit? Fruit has natural sugars, so on one hand we may suspect it is a risk factor for diabetes. On the other hand, fruit is considered by many to be a healthy food, which may lead us to suspect that it is protective against diabetes. Let’s see what the data shows.\n\ndm_data |&gt;\n  group_by(Diabetes, Fruits) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes [2]\n  Diabetes Fruits  count\n  &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;\n1 no       no      78129\n2 no       yes    140205\n3 yes      no      14653\n4 yes      yes     20693\n\n\nAgain, a contingency table is not the easiest to interpret in this case since the counts of diabetes vs no diabetes are lopsided. As before, we will take a look at a bar plot, which is more interpretable.\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = Fruits), position = 'dodge') + labs(x = 'Diabetes Status', title = 'Diabetes Status by Fruit Consumption')\n\n\n\n\n\n\n\n\nWe see that if you don’t have diabetes, you are much more likely to be a fruit eater. This could suggest fruit is protective against diabetes. Alternatively, it could suggest that people who are generally healthier eat more fruit and happen to also be at lower risk for diabetes because of a healthy lifestyle. A third possible explanation is that people with diabetes are more likely to AVOID fruit because it makes their blood sugar spike and hard to control.\nLet’s create the same plot but facet by vegetable consumption.\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = Fruits), position = 'dodge') + facet_wrap(~Veggies) + labs(x = 'Diabetes Status', title = 'Diabetes Status by Fruit and Vegetable Consumption') \n\n\n\n\n\n\n\n\nWe see that for those who eat vegetables, they are more likely to eat fruit than not, regardless of diabetes status. And in those who don’t eat vegetables, they are more likely to NOT eat fruit, regardless of diabetes status. This suggests fruit and vegetable consumption are associated, supporting the idea that people who eat fruits are probably generally healthier, as they eat vegetables too, regardless of diabetes status.\nNext we look at physical activity and diabetes status. Again we will create both a contingency table and a bar plot, expecting the bar plot to be easier to interpret.\n\ndm_data |&gt;\n  group_by(Diabetes, PhysActivity) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes [2]\n  Diabetes PhysActivity  count\n  &lt;fct&gt;    &lt;fct&gt;         &lt;int&gt;\n1 no       no            48701\n2 no       yes          169633\n3 yes      no            13059\n4 yes      yes           22287\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = PhysActivity), position = 'dodge') + labs(x = 'Diabetes Status', title = 'Diabetes Status by Physical Activity')\n\n\n\n\n\n\n\n\nIt seems that people are more likely than not to have done physical activity in the past 30 days, regardless of diabetes status. It also appears that in those without diabetes, a much larger proportion of people are physically active compared with the group having diabetes. This suggests a relationship between physical activity and diabetes. As noted with other previously discussed variables, we cannot tell whether physical activity is causal for diabetes, the other way around, or whether neither plays a causal role in the other and that unconsidered confounding variables could explain the observed relationship between the variables. Let’s facet by difficulty walking and see if the relationships change.\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = PhysActivity), position = 'dodge') + facet_wrap(~DiffWalk) + labs(x = 'Diabetes Status', title = 'Diabetes Status by Physical Activity and Difficulty Walking')\n\n\n\n\n\n\n\n\nIn those who don’t have difficulty walking, the plot looks similar to the unfaceted plot, except more diabetics are physically active. In those who do have difficulty walking, activity rates are similar between those with diabetes and those without diabetes. This suggests that physical inactivity alone is unlikely to cause diabetes!\nThe last thing we will look at is diabetes rates by sex.\n\ndm_data |&gt;\n  group_by(Sex, Diabetes) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 3\n# Groups:   Sex [2]\n  Sex    Diabetes  count\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 female no       123563\n2 female yes       18411\n3 male   no        94771\n4 male   yes       16935\n\ndm_data |&gt;\n  ggplot(aes(Diabetes)) + geom_bar(aes(fill = Sex), position = 'dodge') + labs(x = 'Diabetes Status', title = 'Diabetes Status by Sex')\n\n\n\n\n\n\n\n\nThere are clearly more females than males in this data set, since for both diabetes and non-diabetes groups there are more females than males. The difference between males and females is more pronounced in the non-diabetes group, suggesting perhaps that males are more likely to have diabetes than females. To see this a little bit better, we can use a stacked bar chart with normalized height instead.\n\ndm_data |&gt;\n  ggplot(aes(Sex)) + geom_bar(aes(fill = Diabetes), position = 'fill') + labs(y = 'Proportion', title = 'Diabetes Rates by Sex')\n\n\n\n\n\n\n\n\nThis chart makes it clear that males in this data set are slightly more likely to have diabetes than females in this data set. Because the difference is small and this is only one random sample of the population, this result does not necessarily imply that the true population rates of diabetes are different in males vs females.\nThis concludes our EDA. To head over to the webpage for viewing our modeling on the data set, click the following link: https://bjrabe.github.io/final_project/Modeling.html"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "Introduction",
    "text": "Introduction\nOn this page, we create two models (a classification tree and a random forest) which model the response variable diabetes status (categorical, levels yes and no) on the explanatory variables we explored on the EDA webpage. If you haven’t seen it, head over to https://bjrabe.github.io/final_project/EDA.html. The following provides a list of explanatory variables we will use, a description of each variable, and whether the variable is quantitative or categorical:\n\nBMI: Body Mass Index, which is weight in kg divided by the square of height in meters (quantitative)\nSmoker: Smoked 100 or more cigarettes in lifetime (categorical, levels yes and no)\nPhysActivity: Physical activity in past 30 days, not including job (categorical, levels yes and no)\nFruits: Consume fruit one or more times per day (categorical, levels yes and no)\nVeggies: Consume vegetables one or more times per day (categorical, levels yes and no)\nHvyAlcoholConsump: 14 or more alcoholic drinks per week for a male and 7 or more drinks per week for a female (categorical, levels yes and no)\nDiffWalk: Serious difficulty walking or climbing stairs (categorical, levels yes and no)\nSex: (categorical, levels male and female)\n\nWe will accomplish our task by splitting the data into training and test sets (70/30 split), fitting the models on the training set using 5-fold cross validation to find the best tuning parameters for each model, and then comparing the two models on the test set."
  },
  {
    "objectID": "Modeling.html#data",
    "href": "Modeling.html#data",
    "title": "Modeling",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\nIn this section, we will fit a classification tree to predict presence or absence of diabetes using the explanatory variables outlined above. Before we get into this, we discuss what a classification tree is. We are only discussing the classification case since this is what we are doing in this project, but a decision tree can be created for a regression problem as well, in which case it is called a regression tree.\nA classification tree is a modeling technique that, once fit, makes a prediction by following branches of a tree according to values of predictor inputs. In essence, it is like following a flowchart to make a decision based on seeing which conditions apply at each branch. The tree is fit by dividing the predictor space into different regions. Then on each region, a different prediction is made. The regions are created through recursive binary splitting, a greedy algorithm. By greedy algorithm, we mean one that considers the optimal choice at only the single step under consideration, without considering other steps or how they may influence the model overall. Recursive binary splitting occurs by considering each data point in the region of predictor space under consideration (called a node) and choosing the point for which, after splitting the data in two on either side of the point, the sum of values of the loss function as calculated on either side of the point is minimized. The new regions formed on either side of this point are called nodes. The process is repeated in each node, which creates new nodes inside of the original nodes. The process continues in this way inside each new node until a specified number of branches have been formed or until each node no longer contains a specified minimum number of points required to continue breaking up that node into smaller nodes.\nNow that we have discussed what a classification tree is, we will fit one to our training data. We use 5-fold cross validation to find the best value of our tuning parameter(s) and then fit the best model to the entire training set. We will tune on the cost complexity parameter and choose our own values for tree depth and minimum number of points per node to continue branching (the argument for this parameter is min_n).\n\n### create model ----\ntree_mod &lt;- decision_tree(cost_complexity = tune(),\n                           tree_depth = 5,\n                           min_n = 10) |&gt;\n  set_engine('rpart') |&gt;\n  set_mode('classification')\n\n### create workflow ---\ntree_wkfl &lt;- workflow() |&gt;\n  add_recipe(dm_rec) |&gt;\n  add_model(tree_mod) \n\n### tune cost complexity using 5-fold cv ---\ntree_grid &lt;- tree_wkfl |&gt;\n  tune_grid(resamples = dm_5_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 10),\n            metrics = metric_set(mn_log_loss))\n\n### extract best parameter value for cost complexity ---\ntree_param_best &lt;- tree_grid |&gt;\n  select_best(metric = 'mn_log_loss')\n\n### fit model to whole training set ---\ntree_final &lt;- tree_wkfl |&gt;\n  finalize_workflow(tree_param_best) |&gt;\n  last_fit(dm_split, metrics = metric_set(mn_log_loss))\n\nNow our best classification tree model, fit to the entire training set, is stored in the object tree_final. We will use this object in a later step."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "Random Forest",
    "text": "Random Forest\nIn this section, we will fit a random forest to predict presence or absence of diabetes using the explanatory variables outlined above. Before we get into this, we discuss what a random forest model is. We are only discussing the classification case since this is what we are doing in this project, but a random forest can be created for a regression problem as well.\nTo explain a random forest, we start with the classification tree discussed above. We begin by creating B different resamples from the data with replacement (called bootstrap resamples), and we fit a classification tree to each of the B bootstrap resamples. So we are creating B different trees. The reason we do this is that classification trees (and regression trees) are very sensitive to changes in the data. So you may get a completely different model and completely different predictions with only small changes in the data used to train the model. In technical terms, single tree models have a high variance. By creating B different trees and considering all of them when making predictions, we can reduce the variance in our model. However, in random forests particularly, when creating each split (branch) of a given tree, we are not considering all of the explanatory variables. Instead, we take a random subset of the predictors (explanatory variables) at each step and only consider those when determining a branch point. The branch is determined and when creating the next split (branch) inside our new nodes, we consider a different random subset of the explanatory variables to be used to determine the branch point. The reason for doing this is that it reduces correlation between the B different trees we are fitting. The idea is that if any of the predictors are very strong, they will be featured in most of the B trees, which makes the trees highly correlated. When the trees are correlated, we don’t achieve the desired reduction in variance we are targeting by fitting different trees on the different bootstrap resamples. By considering only a subset of predictors at each step, we are ensuring the trees are not dominated by only a few strong predictors. Thus, there is less correlation between trees and a greater reduction of variance in our estimates from the model. Once we have our B different trees, we select our predicted class by choosing the most common class predicted among the B different trees from the given input values (values of the explanatory variables).\nNow that we have discussed what a random forest is, we will fit one to our training data. We use 5-fold cross validation to find the best value of our tuning parameter(s) and then fit the best model to the entire training set. We will tune on the number of predictors that will be randomly sampled at each split when creating the tree models (parameter is called mtry). When setting up a grid of tuning parameters, we could have R create one for us using the grid_regular() function as we did for the classification tree above. However, the mtry parameter is simple in the sense that we have 8 predictors, so at each split in each tree we must be selecting somewhere between 1 and 8 predictors for binary fitting at that step. Therefore in this case it is just as easy to create our own tuning grid with the candidate parameter values being the integers 1,2,…,8.\nWe will also limit the number of trees to 100 to avoid impractically long compute time. The other parameters will be chosen by the software.\n\n### create a model ---\nrf_mod &lt;- rand_forest(mtry = tune(), trees = 100) |&gt;\n  set_engine('ranger') |&gt;\n  set_mode('classification')\n\n### create a workflow ---\nrf_wkfl &lt;- workflow() |&gt;\n  add_recipe(dm_rec) |&gt;\n  add_model(rf_mod)\n\n### manually create a grid for tuning ---\ngrid_manual &lt;- data.frame('mtry' = 1:8)\n\n### tune mtry using 5-fold cv ---\nrf_grid &lt;- rf_wkfl |&gt;\n  tune_grid(resamples = dm_5_fold,\n            grid = grid_manual,\n            metrics = metric_set(mn_log_loss))\n\n### extract best parameter value for mtry ---\nrf_param_best &lt;- rf_grid |&gt;\n  select_best(metric = 'mn_log_loss')\n\n### fit model to whole training set ---\nrf_final &lt;- rf_wkfl |&gt;\n  finalize_workflow(rf_param_best) |&gt;\n  last_fit(dm_split, metrics = metric_set(mn_log_loss))\n\nNow our best random forest model, fit to the entire training set, is stored in the object rf_final. We will use this object, along with tree_final, in the next step."
  },
  {
    "objectID": "Modeling.html#data-and-common-steps",
    "href": "Modeling.html#data-and-common-steps",
    "title": "Modeling",
    "section": "Data and Common Steps",
    "text": "Data and Common Steps\nIn this section, we will perform all steps which are common to creating both the classification tree and random forest models. We use the tidymodels framework for this purpose.\nFirst, we read in the data.\n\ndm_data &lt;- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNext, we note that our categorical variables are saved as data type double. We will change these to factors with the appropriate levels (discussed above).\n\ndm_data &lt;- dm_data |&gt;\n  mutate(Diabetes = factor(Diabetes_binary, levels = c(0, 1), labels = c('no', 'yes')),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c('no', 'yes')),\n         PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c('no', 'yes')),\n         Fruits = factor(Fruits, levels = c(0,1), labels = c('no', 'yes')),\n         Veggies = factor(Veggies, levels = c(0,1), labels = c('no', 'yes')),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c('no', 'yes')),\n         DiffWalk = factor(DiffWalk, levels = c(0,1), labels = c('no', 'yes')),\n         Sex = factor(Sex, levels = c(0,1), labels = c('female', 'male'))\n         )\n\nNext we check for missing values.\n\nis.na(dm_data) |&gt;\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income             Diabetes \n                   0                    0 \n\n\nFortunately we see there is no missing data.\nNow we can split the data into training and test sets. As discussed above, we use a 70/30 training/test split. We set a seed to ensure reproducibility.\n\nset.seed(1993)\n\ndm_split &lt;- initial_split(dm_data, prop = 0.7)\ndm_train &lt;- training(dm_split)\ndm_test &lt;- testing(dm_split)\n\nNow that we have split our data, we want to create 5 folds on our training set to be used in 5-fold cross validation.\n\ndm_5_fold &lt;- vfold_cv(dm_train, 5)\n\nNow we are ready to make our recipe, which will be common to both the classification tree and random forest models. Note we do not need to normalize our numeric predictors for either a classification tree or a random forest.\n\ndm_rec &lt;- recipe(Diabetes ~ BMI + Smoker + PhysActivity + Fruits + Veggies + HvyAlcoholConsump + DiffWalk + Sex, data = dm_train) |&gt;\n  step_dummy(all_factor(), -Diabetes)"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nIn this section, we compare our classification tree model and random forest model to see which performs better on the test set. The metric we are using is log loss.\n\nrbind(tree_final |&gt; collect_metrics(),\n      rf_final |&gt; collect_metrics()) |&gt;\n  mutate('model' = c('tree', 'random_forest')) |&gt;\n  select(model, .metric, .estimate)\n\n# A tibble: 2 × 3\n  model         .metric     .estimate\n  &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n1 tree          mn_log_loss     0.381\n2 random_forest mn_log_loss     0.361\n\n\nWe see that the random forest model has a lower value for log loss on the test set. Therefore, we will choose the random forest model as our “best” model!\nIn our API, we are going to fit our “best” model to the entire data set. In order to do this, we need to know what the tuned value is for our parameter mtry.\n\nrf_param_best\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     3 pre0_mod3_post0\n\n\nIt looks like the optimal value is 3, which means that during the fitting of the random forest model to the full data set in the API, we should be considering a random sample of 3 of the available 8 predictors at each step of the recursive binary splitting algorithm (described above). Since we have already found this value, we can just set the mtry argument to 3 when it comes time to fit the model to the whole data set."
  }
]